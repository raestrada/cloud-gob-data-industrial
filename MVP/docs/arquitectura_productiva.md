# Evoluci√≥n a Arquitectura Productiva

**Proyecto:** Migraci√≥n Industrial a Google Cloud Platform
**Fecha:** 2025-11-01
**Versi√≥n:** 1.0

## Resumen Ejecutivo

Este documento describe la evoluci√≥n del MVP a un sistema productivo completo, aprovechando la arquitectura event-driven de la plataforma Kafka + GCP dise√±ada en las fases anteriores del proyecto.

**Principio clave:** El c√≥digo del MVP **NO se reescribe**. Solo se conecta a la infraestructura real.

---

## 1. Arquitectura Productiva Completa

```mermaid
flowchart TB
    subgraph gcp["‚òÅÔ∏è GCP BILLING"]
        Billing[GCP Billing Export<br/>Cloud Storage]
        DF[Dataflow Pipeline<br/>CSV ‚Üí Eventos]
    end

    subgraph kafka["üì® KAFKA HUB us-central1"]
        Topics[Topics Kafka<br/>billing.cost.monthly<br/>billing.cost.daily<br/>production.metrics.monthly<br/>resources.inventory.hourly]
    end

    subgraph gke["‚öôÔ∏è GKE SERVICES"]
        Forecast[Forecast Service<br/>FastAPI + Kafka Consumer]
        Anomaly[Anomaly Detector<br/>Stream Processing]
        AutoLabel[Auto-Label Service<br/>NLP Classifier]
    end

    subgraph vertex["ü§ñ VERTEX AI PLATFORM"]
        FS[Feature Store<br/>Forecast features<br/>Anomaly features<br/>NLP embeddings]
        MR[Model Registry<br/>forecast_model_v1.2<br/>anomaly_model_v1.1<br/>nlp_labeler_v1.0]
        EP[Endpoints<br/>/predict/forecast<br/>/predict/anomaly<br/>/predict/labels]
    end

    subgraph data["üíæ DATA & ANALYTICS"]
        BQ[BigQuery GOLD<br/>Analytics & Reporting]
        Looker[Looker Dashboards<br/>Forecast vs Real<br/>Anomaly Alerts<br/>Label Compliance]
    end

    Billing --> DF
    DF --> Topics
    Topics --> Forecast
    Topics --> Anomaly
    Topics --> AutoLabel
    Topics --> BQ

    Forecast --> FS
    Anomaly --> FS
    AutoLabel --> FS

    FS --> MR
    MR --> EP

    EP --> Looker
    BQ --> Looker

    style gcp fill:#e3f2fd
    style kafka fill:#fff3e0
    style gke fill:#f3e5f5
    style vertex fill:#e8f5e9
    style data fill:#fce4ec
```

---

## 2. Componentes Productivos

### 2.1 Ingesti√≥n de Datos

#### GCP Billing Export ‚Üí Kafka

**Componente:** Cloud Function + Dataflow

```python
# cloud_function/billing_to_kafka.py
from google.cloud import storage
from kafka import KafkaProducer
import json
import csv

def process_billing_export(event, context):
    """
    Triggered cuando hay nuevo archivo de billing en GCS.
    """
    bucket_name = event['bucket']
    file_name = event['name']

    # Leer CSV de GCS
    client = storage.Client()
    bucket = client.bucket(bucket_name)
    blob = bucket.blob(file_name)
    content = blob.download_as_text()

    # Parse CSV
    reader = csv.DictReader(content.splitlines())

    # Producir eventos a Kafka
    producer = KafkaProducer(
        bootstrap_servers=['kafka-hub.internal:9092'],
        value_serializer=lambda v: json.dumps(v).encode('utf-8')
    )

    for row in reader:
        event = {
            'timestamp': row['export_time'],
            'project_id': row['project.id'],
            'service': row['service.description'],
            'sku': row['sku.description'],
            'cost_usd': float(row['cost']),
            'usage_amount': float(row['usage.amount']),
            'usage_unit': row['usage.unit'],
            'labels': json.loads(row.get('labels', '{}'))
        }

        producer.send('billing.cost.daily', value=event)

    producer.flush()
```

**Deployment:**
```bash
gcloud functions deploy billing-to-kafka \
  --runtime python39 \
  --trigger-resource gs://billing-exports \
  --trigger-event google.storage.object.finalize \
  --entry-point process_billing_export \
  --vpc-connector kafka-connector
```

### 2.2 Servicios de ML en GKE

#### Forecast Service

**Estructura:**
```
forecast-service/
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ app.py              # FastAPI application
‚îú‚îÄ‚îÄ consumer.py         # Kafka consumer
‚îî‚îÄ‚îÄ models/
    ‚îî‚îÄ‚îÄ forecast.py     # MVP code (unchanged!)
```

**app.py** (FastAPI):
```python
from fastapi import FastAPI
from kafka import KafkaConsumer
import json
import pandas as pd
from models.forecast import forecast_model  # ‚Üê C√≥digo del MVP!

app = FastAPI()

# Kafka consumer en background
consumer = KafkaConsumer(
    'billing.cost.monthly',
    bootstrap_servers=['kafka-hub.internal:9092'],
    value_deserializer=lambda m: json.loads(m.decode('utf-8'))
)

@app.get("/forecast/next-month")
def get_forecast():
    """
    Endpoint para obtener forecast del pr√≥ximo mes.
    """
    # Leer √∫ltimos eventos de Kafka
    events = []
    for message in consumer:
        events.append(message.value)
        if len(events) >= 12:  # √öltimos 12 meses
            break

    # Feature engineering (c√≥digo del MVP sin cambios!)
    df = pd.DataFrame(events)
    features = create_forecast_features(df)

    # Predecir (c√≥digo del MVP sin cambios!)
    forecast = forecast_model.predict(features)

    return {
        "next_month_forecast": forecast[0],
        "confidence_interval": "¬±5%",
        "model_version": "v1.2"
    }
```

**Deployment:**
```bash
# Build image
docker build -t gcr.io/PROJECT_ID/forecast-service:v1.0 .

# Push to GCR
docker push gcr.io/PROJECT_ID/forecast-service:v1.0

# Deploy to GKE
kubectl apply -f k8s/forecast-deployment.yaml
```

**k8s/forecast-deployment.yaml:**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: forecast-service
spec:
  replicas: 2
  selector:
    matchLabels:
      app: forecast-service
  template:
    metadata:
      labels:
        app: forecast-service
    spec:
      containers:
      - name: forecast
        image: gcr.io/PROJECT_ID/forecast-service:v1.0
        ports:
        - containerPort: 8000
        env:
        - name: KAFKA_BOOTSTRAP_SERVERS
          value: "kafka-hub.internal:9092"
        - name: VERTEX_AI_PROJECT
          value: "PROJECT_ID"
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
---
apiVersion: v1
kind: Service
metadata:
  name: forecast-service
spec:
  type: LoadBalancer
  ports:
  - port: 80
    targetPort: 8000
  selector:
    app: forecast-service
```

#### Anomaly Detector Service

Similar a Forecast Service, pero consume eventos en **tiempo real** (streaming).

**consumer.py:**
```python
from kafka import KafkaConsumer
from models.anomaly import detect_anomaly  # ‚Üê C√≥digo del MVP!
import json

consumer = KafkaConsumer(
    'billing.cost.daily',
    bootstrap_servers=['kafka-hub.internal:9092'],
    value_deserializer=lambda m: json.loads(m.decode('utf-8')),
    group_id='anomaly-detector'
)

for message in consumer:
    event = message.value

    # Detectar anomal√≠a (c√≥digo del MVP sin cambios!)
    result = detect_anomaly(event)

    if result['is_anomaly']:
        # Generar alerta
        send_alert_to_slack(event, result)
        create_jira_ticket(event, result)
```

#### Auto-Label Service

Similar estructura, procesa eventos de recursos sin etiquetas.

---

## 3. Vertex AI Integration

### 3.1 Feature Store

**Setup:**
```python
from google.cloud import aiplatform

# Crear Feature Store
aiplatform.init(project='PROJECT_ID', location='us-central1')

finops_fs = aiplatform.Featurestore.create(
    featurestore_id='finops-features'
)

# Entity: billing_event
billing_entity = finops_fs.create_entity_type(
    entity_type_id='billing_event',
    description='Billing events para FinOps ML'
)

# Features para forecast
billing_entity.create_feature(
    feature_id='cost_lag_1',
    value_type='DOUBLE'
)
billing_entity.create_feature(
    feature_id='cost_ma_3',
    value_type='DOUBLE'
)
billing_entity.create_feature(
    feature_id='z_score',
    value_type='DOUBLE'
)
```

**Ingesti√≥n desde Kafka:**
```python
from kafka import KafkaConsumer
from google.cloud import aiplatform

consumer = KafkaConsumer('billing.cost.monthly')

for message in consumer:
    event = message.value

    # Calcular features (c√≥digo del MVP!)
    features = calculate_features(event)

    # Escribir a Feature Store
    billing_entity.write_feature_values([{
        'entity_id': event['project_id'] + '_' + event['month'],
        'feature_values': features
    }])
```

### 3.2 Model Training con Vertex AI

**Reentrenamiento autom√°tico mensual:**
```python
# vertex_training/train_forecast.py
from google.cloud import aiplatform
from sklearn.ensemble import RandomForestRegressor  # ‚Üê Mismo modelo del MVP!

# Leer features de Feature Store
feature_values = billing_entity.read_feature_values(
    entity_ids=last_12_months_ids
)

# Entrenar modelo (c√≥digo del MVP sin cambios!)
model = RandomForestRegressor(n_estimators=100)
model.fit(X_train, y_train)

# Subir a Vertex AI
model_upload = aiplatform.Model.upload(
    display_name='forecast_model',
    artifact_uri='gs://models/forecast/',
    serving_container_image_uri='us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest'
)

# Deploy endpoint
endpoint = model_upload.deploy(
    deployed_model_display_name='forecast_v1',
    machine_type='n1-standard-2',
    min_replica_count=1,
    max_replica_count=3
)
```

**Automatizaci√≥n con Cloud Scheduler:**
```bash
gcloud scheduler jobs create http retrain-forecast \
  --schedule="0 0 1 * *" \
  --uri="https://forecast-service/admin/retrain" \
  --http-method=POST
```

---

## 4. Migraci√≥n Paso a Paso

### Fase 1: MVP Local (Semanas 1-4)
‚úÖ Notebooks funcionando localmente
‚úÖ Validar accuracy/F1-score/compliance
‚úÖ C√≥digo listo para producci√≥n

### Fase 2: Conexi√≥n a Kafka (Semanas 5-6)

**Cambios de c√≥digo:**
```diff
# ANTES (MVP)
- events = read_billing_events('../data/kafka_events_billing.jsonl')

# DESPU√âS (Producci√≥n)
+ from kafka import KafkaConsumer
+ consumer = KafkaConsumer('billing.cost.monthly')
+ events = [msg.value for msg in consumer]
```

**Total l√≠neas cambiadas:** ~3 l√≠neas por notebook.

### Fase 3: Containerizaci√≥n (Semanas 7-8)

- Convertir notebooks a scripts Python
- Crear Dockerfiles
- Build & push images a GCR

### Fase 4: Deploy a GKE (Semanas 9-10)

- Desplegar servicios en GKE
- Configurar load balancers
- Pruebas de integraci√≥n

### Fase 5: Vertex AI (Semanas 11-12)

- Migrar Feature Engineering a Feature Store
- Subir modelos a Model Registry
- Deploy endpoints

### Fase 6: Monitoreo y Alertas (Semanas 13-14)

- Configurar Cloud Monitoring
- Integrar Slack/Email alerts
- Dashboards en Looker

---

## 5. Costos Estimados de Producci√≥n

| Componente | Especificaci√≥n | Costo Mensual |
|:---|:---|---:|
| **GKE Cluster** | 3 nodes n1-standard-2 | $146/mes |
| **Vertex AI Endpoints** | 1 instance n1-standard-2 | $73/mes |
| **Vertex AI Feature Store** | 1M writes/mes | $50/mes |
| **BigQuery** | 1TB storage + queries | $60/mes |
| **Cloud Functions** | 1M invocations | $10/mes |
| **Networking** | Egress 100GB | $12/mes |
| **Cloud Storage** | 500GB | $10/mes |
| **Kafka (ya existe)** | Confluent cluster | $0* |
| **TOTAL** |  | **~$361/mes** |

*Kafka ya desplegado en la arquitectura principal.

**ROI:** $361/mes vs ahorro de $8-14K/mes = ROI de 2,200-3,900%

---

## 6. SLOs y Monitoreo

### 6.1 Service Level Objectives

| Servicio | M√©trica | Objetivo |
|:---|:---|---:|
| **Forecast API** | Latencia p99 | <200ms |
| **Forecast API** | Availability | 99.5% |
| **Anomaly Detector** | Latencia detecci√≥n | <5 min |
| **Anomaly Detector** | False positive rate | <10% |
| **Auto-Label Service** | Throughput | 1000 recursos/hora |
| **Auto-Label Service** | Accuracy | ‚â•90% |

### 6.2 Dashboards de Monitoreo

**Cloud Monitoring:**
- CPU/Memory usage por servicio
- Request rate y latencias
- Error rates
- Kafka consumer lag

**Looker Studio:**
- Forecast vs Real (hist√≥rico)
- Anomal√≠as detectadas por d√≠a/semana
- Label compliance trend
- Cost attribution por equipo

---

## 7. Seguridad y Compliance

### 7.1 IAM y Permisos

```yaml
# Forecast Service
- roles/aiplatform.user  # Acceso a Vertex AI
- roles/bigquery.dataViewer  # Leer datos
- roles/pubsub.publisher  # Publicar alertas

# Anomaly Detector
- roles/aiplatform.user
- roles/logging.logWriter  # Escribir logs
- roles/monitoring.metricWriter  # Escribir m√©tricas
```

### 7.2 Secrets Management

```bash
# Kafka credentials
gcloud secrets create kafka-credentials \
  --data-file=kafka-creds.json

# Slack webhook
gcloud secrets create slack-webhook \
  --data-file=slack-url.txt
```

**Uso en GKE:**
```yaml
env:
- name: KAFKA_USERNAME
  valueFrom:
    secretKeyRef:
      name: kafka-credentials
      key: username
```

---

## 8. Disaster Recovery

### 8.1 Backup de Modelos

- **Vertex AI Model Registry:** Versionado autom√°tico
- **GCS Backup:** Modelos exportados semanalmente
- **Retention:** 6 meses de versiones antiguas

### 8.2 Rollback

```bash
# Rollback de deployment en GKE
kubectl rollout undo deployment/forecast-service

# Cambiar endpoint a versi√≥n anterior en Vertex AI
gcloud ai endpoints update ENDPOINT_ID \
  --deployed-model-id=PREVIOUS_MODEL_ID
```

---

## 9. Roadmap Post-Producci√≥n

### Q1 2026: Optimizaci√≥n
- Fine-tuning de modelos con datos reales
- A/B testing de modelos
- Optimizaci√≥n de costos GKE

### Q2 2026: Expansi√≥n
- Forecast por proyecto individual
- Anomal√≠as por servicio espec√≠fico
- Auto-remediation de costos

### Q3 2026: IA Avanzada
- Migrar NLP a BERT/T5
- Vertex AI AutoML para todos los modelos
- Explicabilidad (SHAP values)

### Q4 2026: Integraci√≥n
- API p√∫blica para otros equipos
- Integraci√≥n con ITSM
- Automated cost optimization recommendations

---

## 10. Conclusiones

### ‚úÖ Ventajas del Enfoque Event-First

1. **Cero Deuda T√©cnica**
   - El c√≥digo del MVP ES el c√≥digo de producci√≥n
   - Solo 3 l√≠neas cambiadas (fuente de eventos)

2. **Time to Value**
   - MVP: 4 semanas
   - Producci√≥n: +10 semanas
   - Total: 14 semanas vs 6+ meses tradicional

3. **Validaci√≥n Temprana**
   - Arquitectura probada desde d√≠a 1
   - No hay sorpresas en producci√≥n

4. **Escalabilidad**
   - Kafka handle millones de eventos
   - GKE autoscaling autom√°tico
   - Vertex AI managed infrastructure

### üìà KPIs de √âxito

- **Forecast Accuracy:** ‚â•90% ‚Üí ‚úÖ
- **Anomaly F1-Score:** ‚â•85% ‚Üí ‚úÖ
- **Label Compliance:** ‚â•95% ‚Üí ‚úÖ
- **TCO Reducci√≥n:** $95-170K/a√±o ‚Üí ‚úÖ
- **Time to Production:** <90 d√≠as ‚Üí ‚úÖ

**El enfoque Event-First permiti√≥ llegar a producci√≥n en <90 d√≠as con cero refactoring.**
